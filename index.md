# Data description
The research question I am trying to answer is what temperatures with regards to the wall temp and bulk sodium temp, the optical fiber temperature sensors will experience in a liquid sodium wire wrapped test section. There are two separate parts of my research, first is understanding what temperatures will be recorded by the fiber temperature sensors in the helical geometry vs what temperature they will be experiencing. Secondly, I will be conducting Computational Fluid Dynamics simulations to predict what temperatures will be experienced. Since there is no experimental data to validate against, I will be conducting a sensitivity analysis to understand what parameters the system is sensitive to.  This is to support the Sodium Flow Investigative Experiment (SOFIE) facility being built at OSU. This liquid sodium loop is being built to validate thermal hydraulic codes that will assist in licensing the Natrium reactor by terrapower. My project will help ensure that the data generated by the facility is accurate and the nature of what the data means is understood. 
I am using a few different types of data. Mostly I will be collecting temperature measurements from the optical fibers saved to a TSV file. I will be comparing them to Thermocouple temperature measurements also in a TSV format for the first part. The second part will consist of several different data types such as temperature, Nusselt number, Prandtl number. I am still not 100% sure of all the parameters that could be impactful for my system but they will be of the sort. There will be images of the simulations as well as plots on the mesh quality and temperature data. The first data types are experimental data, while the second data types are simulation data. The methods I will be using to collect data are a LUNA interrogator system for the optical fiber temperature measurements, a NI-DAQ system for the TC measurements, and star-ccm+ for the simulations.  With regards to the size of my data, The Thermocouple data is just two points sampled a few times a second so they are only on the order of a couple hundred MB maximum. The Optical fibers can measure hundreds of temperature points multiple times a second, so the data can exceed several GB for those datasets. My Star-ccm+ simulations are generally on the order of 20GB each, and I will be generating several dozen of them, so around 150-200GB of data, so only the base files will be uploaded.




# Roles and responsibilities
 I am the one who is in charge of the DMP since it is not directly being assigned by partners in industry. I am a GRA so I assist in roles for building and facilitating the facility. This follows a very strict NQA-1 data requirements. Since my research is an area where I have identified to ensure that the facility produces quality data, I should be aware of the standards set by NQA-1. I will be performing a parametric study  which will have many different simulations with values changed slightly to understand the effects of certain factors. I have an excel file and have written notes about how I want to structure the file names to disseminate which test each file corresponds to. The data will likely be used by our partners to argue that their data is representative of what we think it is. The experimental part compares thermocouples and optical fiber temperature sensors. I have yet to identify exactly how I want to correlate the thermocouple values to the fiber values. I am the only person on the project so I have a document and notes describing the data management decisions I have made so somebody can feasibly figure out what everything is. My data is not protected because I am not allowed to know the export-controlled geometry of the test section we are building since I would have to sign an NDA. I am given numbers that are in the ballpark but there is nothing in the way of data that needs to be protected. It is unrestrictive. My data is currently stored on a drive called depot that my lab has. The NQA-1 system ensures that all data on the drive is backed up at least twice. This is part of the drive itself since the entire lab is generally NQA-1 certified. I would classify that redundancy as automatic. For the simulations, there are enough similar files that even losing one or two would just make me have to rerun a simulation and change a few values so I am not too worried about that. I think Depot creates two copies that can be accessed.
# Data standards and metadata
I have designed a pretty good plan to keep track of all of the simulation data for my research project. I wrote a document describing what I was doing and what data each part of the research project would generate and how they would be stored. For instance, for my sensitivity matrix testing I have an excel document that specifies all the variables and their values for each matrix test. Each different simulation geometry will have different matrix testing excel files. Changes in the parameters for each geometry will be encompassed within a single excel matrix file. For example if I have a geometry that has a wire contact of .1mm and one with a contact of .8mm, they will have separate data organization but if one simulation has .1mm contact with an inlet velocity of 2m/s and the other also has a .1mm contact with an inlet velocity of 4m/s, they would be under the same matrix with a different number. The file name would be something like, .1mm_contact_matrixTesting_simulationNumber1_5_8_23. This would correspond to a certain geometry and would be the first row of the matrix test. This would correspond in the excel file and all the information on the parameters of the test would be in the first row. 

Each separate matrix simulation will have its own folder that the results and figures are stored in so as to not confuse them. 

For version control, I will generally only keep two versions of each matrix test and then a separate geometry parent simulation that all of the others will stem from. This parent simulation file will have all the setup done on it as far as data postprocessing and data acquisition setup. 

For the experimental data, also have a document explaining the file names and format of the experimental data. For some of the experimental runs I have found it most efficient to also go by a similar numbering system and generate an excel table that describes what happened in each test. For example, I am testing fibers in a furnace. For some of the tests I hold them at high temperatures for a while but in other tests I am holding at multiple temperatures to generate a calibration curve. It is easiest for me if I describe each test in an excel table and then just name the test which row of the table it represents. I name it, helical wrap fiber experimental matrix test1(5-8-23) which would then correspond to the excel file name and which test in that excel file. 

  
![image](https://github.com/Svietz1/Grad521-StefanVietzDMP/assets/132002288/ec0364f1-580e-4a62-8f9d-b4529de2eecc)

![image](https://github.com/Svietz1/Grad521-StefanVietzDMP/assets/132002288/015cbdd2-e614-49de-b28e-8a5ddd67eb0c)

The excel file is not yet complete, but will be attached here when it is done


# Storage and security
There are factors with some of my datasets that will limit my availability to share it. The exact dimensions and geometry of my pin wrap bundle are export controlled since they are specific to TerraPower's Natrium reactor. However, I am not using the exact same geometry but a very similar geometry. I will honestly have to ask if I will be able to share that whole data set or just parts of it such as temperatures and velocity profiles. If I am able to share the 7 pin bundle geometry then I will just upload my entire simulation file along with the files that I used to generate data. If i am not able to share the exact geometry, I will just have to share the data that comes from the simulation that are in CSV format.  There will also be a document in the repository explaining how to use the simulation file if necessary. The other two data sets will be freely available since they do not contain anything that could be controlled or harmful since it does not include human subject research. For my single pin sensitivity analysis it just has a single pin with a wire wrapped around it with the same pitch that my experimental setup has.  Since there will be at least two different geometries, each of these will have separate folders with their own excel sheets and base geometry star ccm simulation file. There is not really a better file type to store star-ccm  Then in a separate repository the experimental setup will be shown, as well as geometry and figures describing it. Each test will be described such as 550C_steady_state_test, describing what test the data represents. Since there are 4 different instrumentation types for each test, it will be best to separate the tests and have a different file for every test that was done that contains all the different instrument files. These are all in .csv format, which is how they will be preserved. 
# Access and data sharing
Data will be uploaded to GITHUB when possible
# Archiving and preservation
The data will be archived and preserved in GitHub as well as in my lab group's Depot. This Depot is double backed up for the NQA-1 standard that requires indefinite storage of data. It will obviously not be as accessible in the depot drive as it will be on GitHub, but it will be backed up on depot. The star-ccm files will be maintained in such a way that the base geometry simulation file will be saved, along with the excel file that describes the important boundary conditions and settings that were changed for the sensitivity analysis.
Repository with source code [here](https://github.com/clarallebot/GRAD521_DMPtemplate)
